{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "460b7008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8dc7afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    #Input: X - features of a trainset\n",
    "    #       y - labels of a trainset\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        self.no_of_classes = np.max(self.y_train) + 1\n",
    "     \n",
    "        \n",
    "    #This is our function to calculate all nodes/samples in our radius    \n",
    "    def euclidianDistance(self, Xtest, Xtrain):\n",
    "        return np.sqrt(np.sum(np.power((Xtest - Xtrain), 2)))\n",
    "    \n",
    "       \n",
    "    #our main function is predict\n",
    "    #All calculation is done by using our test or new samples\n",
    "    #There are 4 steps to be performed:\n",
    "    # 1. calculate Prior probability. Ex. P(A) = No_of_elements_of_one_class / total_no_of_samples\n",
    "    # 2. calculate Margin probability P(X) = No_of_elements_in_radius / total_no_of_samples\n",
    "    # 3. calculate Likeliyhood (P(X|A) = No_of_elements_of_current_class / total_no_of_samples\n",
    "    # 4. calculate Posterior probability: P(A|X) = (P(X|A) * P(A)) / P(X)\n",
    "    # NOTE: Do these steps for all clases in dataset!\n",
    "    #\n",
    "    #Inputs: X - test dataset\n",
    "    #       radius - this parameter is how big circle is going to be around our new datapoint, default = 2\n",
    "    def predict(self, X, radius=0.4):   \n",
    "        pred = []\n",
    "        \n",
    "#         number of malignant and benign elements in member_of_class\n",
    "        members_of_class = []\n",
    "        for i in range(self.no_of_classes):\n",
    "            counter = 0\n",
    "            print(i)\n",
    "            for j in range(len(self.y_train)):\n",
    "                if self.y_train[j] == i:\n",
    "                    counter += 1\n",
    "            members_of_class.append(counter)\n",
    "        print(members_of_class)\n",
    "        \n",
    "        #prediction starts\n",
    "        for t in range(len(X)):\n",
    "            #Creating empty list for every class probability\n",
    "            prob_of_classes = []\n",
    "#            for malignant and benign\n",
    "            for i in range(self.no_of_classes):\n",
    "                \n",
    "                #1. step > Prior probability P(class) = no_of_elements_of_that_class/total_no_of_elements\n",
    "                prior_prob = members_of_class[i]/len(self.y_train)\n",
    "\n",
    "                #2. step > Margin probability P(X) = no_of_elements_in_radius/total_no_of_elements\n",
    "                #NOTE: In the same loop collecting infromation for 3. step as well\n",
    "                \n",
    "                inRadius = 0\n",
    "                #counter for how many points are from the current class in circle\n",
    "                inRadius_current_class = 0\n",
    "                \n",
    "#                 finding all points inside the given radius circle\n",
    "                \n",
    "                for j in range(len(self.X_train)):\n",
    "                    if self.euclidianDistance(X[t], self.X_train[j]) < radius:\n",
    "                        inRadius += 1\n",
    "                        if self.y_train[j] == i:\n",
    "                            inRadius_current_class += 1\n",
    "                \n",
    "                #finding margin probability\n",
    "                margin_prob = inRadius/len(self.X_train)\n",
    "                if margin_prob == 0:\n",
    "                    margin_prob = 0.0000000000000000000000000000000000000000000000001\n",
    "                \n",
    "                #3. step > Likelihood P(X|current_class) = no_of_elements_in_circle_of_current_class/total_no_of_elements\n",
    "                likelihood = inRadius_current_class/len(self.X_train)\n",
    "                \n",
    "#                 #4. step > Posterial Probability > formula from Bayes theorem: P(current_class | X) = (likelihood*prior_prob)/margin_prob\n",
    "                post_prob = (likelihood * prior_prob)/margin_prob\n",
    "                prob_of_classes.append(post_prob)\n",
    "            \n",
    "            #Getting index of the biggest element (class with the biggest probability)\n",
    "            pred.append(np.argmax(prob_of_classes))\n",
    "            \n",
    "                \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0524078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_tes, y_pred):\n",
    "    correct = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if(y_tes[i] == y_pred[i]):\n",
    "            correct += 1\n",
    "    return (correct/len(y_tes))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3cbe3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def breastCancerTest():\n",
    "    # Importing the dataset\n",
    "#     dataset = pd.read_csv('breastCancer.csv')\n",
    "# #     dataset.replace('?', 0, inplace=True)\n",
    "# #     dataset = dataset.applymap(np.int64)\n",
    "# #     X = dataset.iloc[:, 1:-1].values    \n",
    "# #     y = dataset.iloc[:,0].values\n",
    "# #     #This part is necessery beacuse of NUMBER of features part of algo\n",
    "# #     #and in this dataset classes are marked with 2 and 4\n",
    "# #     print(y)\n",
    "\n",
    "#     # Splitting the dataset into the Training set and Test set\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "    df = pd.read_csv('data.csv')\n",
    "    \n",
    "#     list of all features from the data\n",
    "    \n",
    "    list_of_columns = list(df.columns)\n",
    "    \n",
    "#     list of features for my model\n",
    "    \n",
    "    prediction_vars = ['diagnosis','radius_mean','perimeter_mean','area_mean','compactness_mean','concavity_mean','concave points_mean',\n",
    "                   'radius_se','area_se' ]\n",
    "    \n",
    "# removing non- required features\n",
    "    \n",
    "    for ele in list_of_columns:\n",
    "        if ele in prediction_vars:\n",
    "            list_of_columns.remove(ele)\n",
    "            \n",
    "    \n",
    "    df = df.drop(columns = list_of_columns ,axis=1)\n",
    "    df['diagnosis'] = df['diagnosis'].map({'M':1, 'B':0})\n",
    "    \n",
    "#     assigning features to x\n",
    "#     assigning labels to y\n",
    "       \n",
    "    X = df.iloc[:,1:].values\n",
    "    y = df.iloc[:,0:1].values\n",
    "    \n",
    "#     splitting data into train and test\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "    \n",
    "\n",
    "#     mean_features = list(df.columns[1:11])\n",
    "#     se_features = list(df.columns[11:21])\n",
    "#     worst_features = list(df.columns[21:31])\n",
    "    \n",
    "#     mean_features.append('diagnosis')\n",
    "#     se_features.append('diagnosis')\n",
    "#     worst_features.append('diagnosis')\n",
    "    \n",
    "    \n",
    "#     train_x = train[prediction_vars]\n",
    "#     train_y = train['diagnosis']\n",
    "#     test_x = test[prediction_vars]\n",
    "#     test_y = test['diagnosis']\n",
    "    \n",
    "    #Testing my Naive Bayes Classifier\n",
    "    NB = NaiveBayesClassifier()\n",
    "    NB.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = NB.predict(X_test, radius=8)\n",
    "    \n",
    "#     sklearn\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    NB_sk = GaussianNB()\n",
    "    NB_sk.fit(X_train, y_train)\n",
    "    \n",
    "    sk_pred = NB_sk.predict(X_test)\n",
    "     \n",
    "    \n",
    "    print(\"Accuracy for my Naive Bayes Classifier: \", accuracy(y_test, y_pred), \"%\")\n",
    "    print(\"Accuracy for sklearn Naive Bayes Classifier: \",accuracy(y_test, sk_pred), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0e89aa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "1\n",
      "[267, 159]\n",
      "Accuracy for my Naive Bayes Classifier:  85.3146853146853 %\n",
      "Accuracy for sklearn Naive Bayes Classifier:  90.20979020979021 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditi Jain\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "breastCancerTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313c28f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576be68d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
